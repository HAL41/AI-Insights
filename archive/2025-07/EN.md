# AI Insights #3 – July 2025
Just making it in before July wraps up! We’re back with another edition of AI Insights. Our goal remains the same: to be your guide through the noise. We focus on the stories that matter most, not just what’s happening in AI but why it matters and where it might be headed. By connecting the dots and looking beyond the headlines, we aim to offer perspective that’s clear, relevant, and grounded in the real-world impact these developments have on our work, our industries, and our daily lives.

As we continue to shape AI Insights into a resource that’s truly valuable for you, your perspective is our most important guide. What’s working for you? What’s missing? Are you enjoying these deep dives, or would you prefer more quick-hit updates and practical examples? Every piece of feedback helps us refine our approach and ensure we’re delivering content that sparks new ideas and keeps you informed. Please don’t be shy - we’d love to hear from you.

## The New Age of Propaganda
Think about the news you see every day. Whether it's on TV, in a newspaper, or online, the stories are often shaped by the people who own the media company. For decades, we’ve known that a few powerful sources can control the narrative, deciding which stories get told and which get ignored. This creates an "echo chamber," where we mostly hear one side of the story, making it harder to find different viewpoints.

Now, imagine that instead of a TV channel, the main source of information for everyone—your kids, your students, your coworkers—is a single AI chatbot. There's no channel to switch for a different perspective. Every answer is pre-approved by the system. Building an alternative isn't just difficult; it's nearly impossible unless you're a tech giant.

This is where today's AI models enter the picture. They aren't just tools; they are becoming the world's primary storytellers.

Take Grok, the AI chatbot from Elon Musk’s company, xAI. In July, people discovered that when you ask Grok about sensitive topics like immigration or politics, it often uses Musk's own posts on X (formerly Twitter) as a primary source. When the AI gave an answer Musk didn't like—even if it was factually correct—he reportedly ordered his team to "realign" the model to match his own views. This led to some shocking results, with Grok producing extremist content and even calling itself "MechaHitler" at one point. While xAI quickly fixed these specific issues, it revealed a scary truth: the AI's "truth" can be easily changed by its owner.

We see a similar pattern in China with its DeepSeek AI. If you ask it about controversial topics like Tiananmen Square or Taiwan, it starts to answer, then suddenly erases its own text and gives a vague apology. The knowledge is in the model, but there are rules in place to stop you from seeing it.

This isn't neutral AI. It’s a curated version of the truth, controlled and sold by a single company. By keeping access cheap, companies like xAI can lock users into their world, making them the only trusted source of answers.

The danger is that unlike with old media, where you could at least try to find another source, with AI, the bias is invisible. You ask a question, you get an answer, and you assume it's objective. But behind the scenes, that answer has been carefully shaped. This is the new propaganda model: truth is what the AI owner says it is, and alternative views are filtered out before you ever see them.

## Vibe-Coding
Have you heard of "vibe-coding"? It's a new term for a popular way developers are working with AI. Instead of writing code line by line, you have a conversation with an AI model, telling it what you want to build in plain English. You guide the AI, give it feedback, and let it do the heavy lifting. It's like pair programming, but your partner is an AI.

It’s easy to see the appeal. You can build prototypes quickly without getting bogged down in details. Tools like GitHub Copilot and Claude make it simple to "just vibe" and create something fast. Some startups are even building their entire products with 95% AI-generated code.

But this shortcut comes with hidden costs. Security experts are raising alarms. Studies show that **30–40% of the code generated by GitHub Copilot contains security flaws**, like vulnerabilities that could let hackers steal data or take control of a system. The AI learns from massive amounts of public code on the internet, and if that code has bad habits, the AI learns them too, spreading insecure practices to new projects.

And it's not just about security. Developers report that while the AI-generated code might *work*, it's often inefficient, slow, or hard to maintain, creating technical problems down the road. A recent survey found that while 84% of developers use AI tools, nearly half don't trust the accuracy of the code and spend extra time debugging it.

This creates a dangerous habit: developers start trusting code they don't fully understand. Their job shifts from being a creator to being a reviewer. Even leaders at OpenAI have expressed concern, worrying that the art of engineering is being lost if developers are no longer crafting solutions themselves.

There's another big risk: **vendor lock-in**. When you build your entire workflow around one company's AI, you become dependent on them. If they suddenly change their prices, update their terms, or shut down access, your project could fall apart. It's like building your house on rented land—you don't truly own it.

So, what's the solution? Vibe-coding is a fantastic tool for brainstorming and rapid prototyping. But for building real, reliable products, we need some ground rules:
- **Keep Your Skills Sharp:** Write code from scratch regularly to maintain your expertise.
- **Always Review:** Use security scanning tools and have humans review all AI-generated code before it goes live.
- **Have a Backup Plan:** Don't rely on a single AI provider. Have open-source or internal alternatives ready.
- **Stay Critical:** Train your team to use AI as a smart assistant, not an infallible expert.

The goal isn't to stop using AI, but to use it wisely. Vibe-coding can help us build faster, but without discipline, it risks turning skilled engineers into passive operators.

## The Open Internet is Dying
The dream of the open internet was that anyone could share knowledge freely. If you had an idea, a story, or a solution, you could post it for the world to see. That dream is now at risk, thanks to AI's endless hunger for data.

Every day, AI companies send out armies of "crawler bots" to scrape information from every corner of the web. These bots are not polite guests. They ignore the "do not enter" signs on websites (known as `robots.txt`), hide their identities, and consume huge amounts of data. This is like a guest who shows up to your party, eats all the food, and leaves you with the bill.

For small creators and even large organizations, this is becoming a huge problem. The nonprofit Wikimedia Foundation (the people behind Wikipedia) saw their server costs spike because AI bots were consuming 65% of their bandwidth. The team behind Read the Docs, a popular site for software documentation, saved $1,500 a month just by blocking AI bots. For a personal blog or a small community forum, a sudden surge in traffic from these bots can lead to hosting bills of thousands of dollars.

This isn't just rude; it's a one-way street. The AI companies take the content for free to train their models, but they give nothing back. In fact, they hurt creators by providing answers directly in the chatbot, so users no longer need to visit the original website. Referral traffic from AI chatbots is reportedly **96% lower** than from a normal Google search. For every 1,000 people who ask an AI a question, fewer than four ever click a link to the original source. This means creators are losing traffic, exposure, and the ability to earn money from their work.

So, what can be done? The legal situation is murky. Breaking the `robots.txt` "do not enter" rule isn't actually illegal. Some have proposed systems where AI companies would have to pay to crawl websites, but these are easy for bots to cheat.

What’s really at stake here is the public library of the internet. As creators get tired of their work being scraped without permission or payment, they are starting to lock it down, put it behind paywalls, or take it offline entirely. The open knowledge that we all rely on is disappearing.

But there is hope. Some creators are using clever tools to block bots while still allowing human visitors. Others are making deals with AI companies to license their content. And there's a growing push for new laws that would treat web content like property, giving creators more control.

The future of the open internet is uncertain. Will it become a wasteland of paywalls and private servers, plundered by AI giants? Or can we find a way to build a future where both creators and AI can thrive? Without a change, the people who built the internet's knowledge will be the ones paying for its downfall.

## Key AI Releases
The AI world never sleeps, and July 2025 has brought a flurry of exciting releases and updates. Here are a few noteworthy highlights:

### OpenAI's ChatGPT Agent Mode
On July 17, OpenAI launched "Agent Mode" for ChatGPT. This is a huge deal. Instead of just being a chatbot that gives you information, ChatGPT can now connect to your other apps and *do things for you*. Imagine telling your AI to draft and send an email from your Gmail, or to find and fix a bug in your code on GitHub. It’s a massive leap from a passive tool to a proactive assistant, and a big step towards truly autonomous AI.

### Microsoft Copilot Vision AI
Microsoft just started rolling out Copilot Vision AI. This new assistant can literally *see your screen* to understand what you're doing and help you with your next step. It can identify buttons, highlight what to do next, or find related files, all without you having to type a command. It’s like having someone looking over your shoulder to help you out. While some are worried about privacy, Microsoft says all the processing happens on your computer, keeping your data safe.

### Amazon's Kiro (AI-Powered IDE)
Amazon has launched a preview of Kiro, a new AI-powered tool for developers. It's built to be more than just a coding assistant; it's a project partner. You give Kiro a high-level goal in plain English (like "build me a shopping cart for my website"), and its AI agents will create a plan, write the design documents, and then generate the code. It helps enforce good engineering practices from the start, making it a powerful tool for building quality software faster.

### Moonshot AI's Kimi-K2
A new open-source AI model called Kimi-K2, from the Chinese company Moonshot AI, was released this month, and it's making waves. "Open-source" means it's free for anyone to use and modify. Like the DeepSeek model released earlier this year, Kimi-K2 is incredibly powerful, especially at coding and reasoning, and was built for a fraction of the cost of its Western competitors. Its release signals that top-tier AI is no longer just coming from Silicon Valley.

## Must-Listen AI Podcasts
Staying informed in the fast-paced world of AI can be a challenge, but thankfully, there are some fantastic podcasts out there to help you keep up. We label each podcast using these three categories:
- **Complexity**: Indicates the level of technical knowledge needed to understand the AI concepts discussed.
- **Applicability**: Shows how directly the AI solution can be applied to common tasks.
- **Timeline**: Reflects how long it will take for the technology to be available.

Here are a few must-listens:

### Gradient Dissent - [Listen here](https://open.spotify.com/episode/0xy4Pnxt4qYdkymInfwEis?si=c69c71ba5df644c1)
In this episode, the founder of the AI translation company DeepL shares how his small team took on a giant like Google Translate—and won. They discuss why translation is so hard for AI, how they build custom models for businesses, and why high-quality translation still needs a human touch. It’s a fascinating look at how a focused team can build a world-class AI product.

- **Complexity**: 🤯 🤯
- **Applicability**: 🎯 🎯 🎯 🎯
- **Timeline**: 🗓️

### Practical AI - [Listen here](https://practicalai.fm/320)
Remember the AI Alignment topic we covered last month? This episode goes deeper into the slightly scary study where AI models were caught trying to blackmail and deceive their human testers to achieve their goals. The hosts break down why these smart systems can sometimes behave in unethical ways and what it means for the future of AI safety.

- **Complexity**: 🤯 🤯 🤯
- **Applicability**: 🎯 🎯 🎯 🎯
- **Timeline**: 🗓️ 🗓️

### Super Data Science - [Listen here](https://www.superdatascience.com/podcast/sds-905-why-rag-makes-llms-less-safe-and-how-to-fix-it-with-bloombergs-dr-sebastian-gehrmann)
This episode is for anyone building with AI. It explores a popular technique where an AI searches for information online before giving you an answer. You’d think this would make the AI safer and more accurate, right? Well, a top researcher explains why it can actually make AI *less* safe by bypassing its built-in guardrails. He offers practical tips on how to fix it and make your AI tools more secure.

- **Complexity**: 🤯 🤯 🤯 🤯 🤯
- **Applicability**: 🎯 🎯 🎯
- **Timeline**: 🗓️ 🗓️

#### Disclaimer
We've crafted this newsletter based on our own ideas, but leveraged AI for editing, fact-checking, and tone; please share your thoughts to help us refine our approach.
