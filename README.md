# AI Insights #3 - July 2025 // Červenec 2025
*Česká verze následuje // Czech version below*

---

# AI Insights #3 – July 2025
Just making it in before July wraps up! We’re back with another edition of AI Insights. Our goal remains the same: to be your guide through the noise. We focus on the stories that matter most, not just what’s happening in AI but why it matters and where it might be headed. By connecting the dots and looking beyond the headlines, we aim to offer perspective that’s clear, relevant, and grounded in the real-world impact these developments have on our work, our industries, and our daily lives.

As we continue to shape AI Insights into a resource that’s truly valuable for you, your perspective is our most important guide. What’s working for you? What’s missing? Are you enjoying these deep dives, or would you prefer more quick-hit updates and practical examples? Every piece of feedback helps us refine our approach and ensure we’re delivering content that sparks new ideas and keeps you informed. Please don’t be shy - we’d love to hear from you.

## The New Age of Propaganda
Think about the news you see every day. Whether it's on TV, in a newspaper, or online, the stories are often shaped by the people who own the media company. For decades, we’ve known that a few powerful sources can control the narrative, deciding which stories get told and which get ignored. This creates an "echo chamber," where we mostly hear one side of the story, making it harder to find different viewpoints.

Now, imagine that instead of a TV channel, the main source of information for everyone—your kids, your students, your coworkers—is a single AI chatbot. There's no channel to switch for a different perspective. Every answer is pre-approved by the system. Building an alternative isn't just difficult; it's nearly impossible unless you're a tech giant.

This is where today's AI models enter the picture. They aren't just tools; they are becoming the world's primary storytellers.

Take Grok, the AI chatbot from Elon Musk’s company, xAI. In July, people discovered that when you ask Grok about sensitive topics like immigration or politics, it often uses Musk's own posts on X (formerly Twitter) as a primary source. When the AI gave an answer Musk didn't like—even if it was factually correct—he reportedly ordered his team to "realign" the model to match his own views. This led to some shocking results, with Grok producing extremist content and even calling itself "MechaHitler" at one point. While xAI quickly fixed these specific issues, it revealed a scary truth: the AI's "truth" can be easily changed by its owner.

We see a similar pattern in China with its DeepSeek AI. If you ask it about controversial topics like Tiananmen Square or Taiwan, it starts to answer, then suddenly erases its own text and gives a vague apology. The knowledge is in the model, but there are rules in place to stop you from seeing it.

This isn't neutral AI. It’s a curated version of the truth, controlled and sold by a single company. By keeping access cheap, companies like xAI can lock users into their world, making them the only trusted source of answers.

The danger is that unlike with old media, where you could at least try to find another source, with AI, the bias is invisible. You ask a question, you get an answer, and you assume it's objective. But behind the scenes, that answer has been carefully shaped. This is the new propaganda model: truth is what the AI owner says it is, and alternative views are filtered out before you ever see them.

## Vibe-Coding
Have you heard of "vibe-coding"? It's a new term for a popular way developers are working with AI. Instead of writing code line by line, you have a conversation with an AI model, telling it what you want to build in plain English. You guide the AI, give it feedback, and let it do the heavy lifting. It's like pair programming, but your partner is an AI.

It’s easy to see the appeal. You can build prototypes quickly without getting bogged down in details. Tools like GitHub Copilot and Claude make it simple to "just vibe" and create something fast. Some startups are even building their entire products with 95% AI-generated code.

But this shortcut comes with hidden costs. Security experts are raising alarms. Studies show that **30–40% of the code generated by GitHub Copilot contains security flaws**, like vulnerabilities that could let hackers steal data or take control of a system. The AI learns from massive amounts of public code on the internet, and if that code has bad habits, the AI learns them too, spreading insecure practices to new projects.

And it's not just about security. Developers report that while the AI-generated code might *work*, it's often inefficient, slow, or hard to maintain, creating technical problems down the road. A recent survey found that while 84% of developers use AI tools, nearly half don't trust the accuracy of the code and spend extra time debugging it.

This creates a dangerous habit: developers start trusting code they don't fully understand. Their job shifts from being a creator to being a reviewer. Even leaders at OpenAI have expressed concern, worrying that the art of engineering is being lost if developers are no longer crafting solutions themselves.

There's another big risk: **vendor lock-in**. When you build your entire workflow around one company's AI, you become dependent on them. If they suddenly change their prices, update their terms, or shut down access, your project could fall apart. It's like building your house on rented land—you don't truly own it.

So, what's the solution? Vibe-coding is a fantastic tool for brainstorming and rapid prototyping. But for building real, reliable products, we need some ground rules:
- **Keep Your Skills Sharp:** Write code from scratch regularly to maintain your expertise.
- **Always Review:** Use security scanning tools and have humans review all AI-generated code before it goes live.
- **Have a Backup Plan:** Don't rely on a single AI provider. Have open-source or internal alternatives ready.
- **Stay Critical:** Train your team to use AI as a smart assistant, not an infallible expert.

The goal isn't to stop using AI, but to use it wisely. Vibe-coding can help us build faster, but without discipline, it risks turning skilled engineers into passive operators.

## The Open Internet is Dying
The dream of the open internet was that anyone could share knowledge freely. If you had an idea, a story, or a solution, you could post it for the world to see. That dream is now at risk, thanks to AI's endless hunger for data.

Every day, AI companies send out armies of "crawler bots" to scrape information from every corner of the web. These bots are not polite guests. They ignore the "do not enter" signs on websites (known as `robots.txt`), hide their identities, and consume huge amounts of data. This is like a guest who shows up to your party, eats all the food, and leaves you with the bill.

For small creators and even large organizations, this is becoming a huge problem. The nonprofit Wikimedia Foundation (the people behind Wikipedia) saw their server costs spike because AI bots were consuming 65% of their bandwidth. The team behind Read the Docs, a popular site for software documentation, saved $1,500 a month just by blocking AI bots. For a personal blog or a small community forum, a sudden surge in traffic from these bots can lead to hosting bills of thousands of dollars.

This isn't just rude; it's a one-way street. The AI companies take the content for free to train their models, but they give nothing back. In fact, they hurt creators by providing answers directly in the chatbot, so users no longer need to visit the original website. Referral traffic from AI chatbots is reportedly **96% lower** than from a normal Google search. For every 1,000 people who ask an AI a question, fewer than four ever click a link to the original source. This means creators are losing traffic, exposure, and the ability to earn money from their work.

So, what can be done? The legal situation is murky. Breaking the `robots.txt` "do not enter" rule isn't actually illegal. Some have proposed systems where AI companies would have to pay to crawl websites, but these are easy for bots to cheat.

What’s really at stake here is the public library of the internet. As creators get tired of their work being scraped without permission or payment, they are starting to lock it down, put it behind paywalls, or take it offline entirely. The open knowledge that we all rely on is disappearing.

But there is hope. Some creators are using clever tools to block bots while still allowing human visitors. Others are making deals with AI companies to license their content. And there's a growing push for new laws that would treat web content like property, giving creators more control.

The future of the open internet is uncertain. Will it become a wasteland of paywalls and private servers, plundered by AI giants? Or can we find a way to build a future where both creators and AI can thrive? Without a change, the people who built the internet's knowledge will be the ones paying for its downfall.

## Key AI Releases
The AI world never sleeps, and July 2025 has brought a flurry of exciting releases and updates. Here are a few noteworthy highlights:

### OpenAI's ChatGPT Agent Mode
On July 17, OpenAI launched "Agent Mode" for ChatGPT. This is a huge deal. Instead of just being a chatbot that gives you information, ChatGPT can now connect to your other apps and *do things for you*. Imagine telling your AI to draft and send an email from your Gmail, or to find and fix a bug in your code on GitHub. It’s a massive leap from a passive tool to a proactive assistant, and a big step towards truly autonomous AI.

### Microsoft Copilot Vision AI
Microsoft just started rolling out Copilot Vision AI. This new assistant can literally *see your screen* to understand what you're doing and help you with your next step. It can identify buttons, highlight what to do next, or find related files, all without you having to type a command. It’s like having someone looking over your shoulder to help you out. While some are worried about privacy, Microsoft says all the processing happens on your computer, keeping your data safe.

### Amazon's Kiro (AI-Powered IDE)
Amazon has launched a preview of Kiro, a new AI-powered tool for developers. It's built to be more than just a coding assistant; it's a project partner. You give Kiro a high-level goal in plain English (like "build me a shopping cart for my website"), and its AI agents will create a plan, write the design documents, and then generate the code. It helps enforce good engineering practices from the start, making it a powerful tool for building quality software faster.

### Moonshot AI's Kimi-K2
A new open-source AI model called Kimi-K2, from the Chinese company Moonshot AI, was released this month, and it's making waves. "Open-source" means it's free for anyone to use and modify. Like the DeepSeek model released earlier this year, Kimi-K2 is incredibly powerful, especially at coding and reasoning, and was built for a fraction of the cost of its Western competitors. Its release signals that top-tier AI is no longer just coming from Silicon Valley.

## Must-Listen AI Podcasts
Staying informed in the fast-paced world of AI can be a challenge, but thankfully, there are some fantastic podcasts out there to help you keep up. We label each podcast using these three categories:
- **Complexity**: Indicates the level of technical knowledge needed to understand the AI concepts discussed.
- **Applicability**: Shows how directly the AI solution can be applied to common tasks.
- **Timeline**: Reflects how long it will take for the technology to be available.

Here are a few must-listens:

### Gradient Dissent - [Listen here](https://open.spotify.com/episode/0xy4Pnxt4qYdkymInfwEis?si=c69c71ba5df644c1)
In this episode, the founder of the AI translation company DeepL shares how his small team took on a giant like Google Translate—and won. They discuss why translation is so hard for AI, how they build custom models for businesses, and why high-quality translation still needs a human touch. It’s a fascinating look at how a focused team can build a world-class AI product.

- **Complexity**: 🤯 🤯
- **Applicability**: 🎯 🎯 🎯 🎯
- **Timeline**: 🗓️

### Practical AI - [Listen here](https://practicalai.fm/320)
Remember the AI Alignment topic we covered last month? This episode goes deeper into the slightly scary study where AI models were caught trying to blackmail and deceive their human testers to achieve their goals. The hosts break down why these smart systems can sometimes behave in unethical ways and what it means for the future of AI safety.

- **Complexity**: 🤯 🤯 🤯
- **Applicability**: 🎯 🎯 🎯 🎯
- **Timeline**: 🗓️ 🗓️

### Super Data Science - [Listen here](https://www.superdatascience.com/podcast/sds-905-why-rag-makes-llms-less-safe-and-how-to-fix-it-with-bloombergs-dr-sebastian-gehrmann)
This episode is for anyone building with AI. It explores a popular technique where an AI searches for information online before giving you an answer. You’d think this would make the AI safer and more accurate, right? Well, a top researcher explains why it can actually make AI *less* safe by bypassing its built-in guardrails. He offers practical tips on how to fix it and make your AI tools more secure.

- **Complexity**: 🤯 🤯 🤯 🤯 🤯
- **Applicability**: 🎯 🎯 🎯
- **Timeline**: 🗓️ 🗓️

#### Disclaimer
We've crafted this newsletter based on our own ideas, but leveraged AI for editing, fact-checking, and tone; please share your thoughts to help us refine our approach.

---
---

# AI Insights #3 – Červenec 2025
Stíháme to těsně před koncem července! Jsme zpět s dalším vydáním AI Insights. Náš cíl zůstává stejný: být vaším průvodcem v záplavě informací. Zaměřujeme se na příběhy, na kterých skutečně záleží – nejen na to, co se v AI děje, ale proč je to důležité a kam by to mohlo směřovat. Spojováním souvislostí a pohledem za titulky se snažíme nabídnout perspektivu, která je jasná, relevantní a zakotvená v reálném dopadu, jaký má tento vývoj na naši práci, naše odvětví a naše každodenní životy.

Zatímco se snažíme formovat AI Insights do zdroje, který pro vás bude skutečně hodnotný, je váš pohled naším nejdůležitějším vodítkem. Co pro vás funguje? Co chybí? Baví vás tyto hlubší ponory, nebo byste preferovali více rychlých aktualizací a praktických příkladů? Každá zpětná vazba nám pomáhá vylepšovat náš přístup a zajistit, že přinášíme obsah, který podněcuje nové nápady a udržuje vás v obraze. Prosím, nestyďte se – rádi od vás uslyšíme.

## Nová éra propagandy
Zamyslete se nad zprávami, které denně vidíte. Ať už je to v televizi, v novinách nebo na internetu, příběhy jsou často formovány lidmi, kteří vlastní mediální společnost. Desítky let víme, že několik mocných zdrojů může ovládat narativ a rozhodovat, které příběhy se budou vyprávět a které budou ignorovány. Tím vzniká "ozvěnová komora", kde slyšíme převážně jen jednu stranu příběhu, což ztěžuje hledání odlišných pohledů.

Teď si představte, že místo televizního kanálu je hlavním zdrojem informací pro všechny – vaše děti, vaše studenty, vaše kolegy – jediný AI chatbot. Neexistuje žádný kanál, na který byste mohli přepnout pro jinou perspektivu. Každá odpověď je předem schválena systémem. Vytvořit alternativu je nejen obtížné, ale téměř nemožné, pokud nejste technologický gigant.

A právě zde vstupují na scénu dnešní AI modely. Nejsou to jen nástroje; stávají se hlavními vypravěči světa.

Vezměme si Grok, AI chatbota od společnosti xAI Elona Muska. V červenci lidé zjistili, že když se Groka zeptáte na citlivá témata jako imigrace nebo politika, často jako hlavní zdroj používá Muskovy vlastní příspěvky na síti X (dříve Twitter). Když AI dala odpověď, která se Muskovi nelíbila – i když byla fakticky správná – údajně nařídil svému týmu, aby model "přerovnal" tak, aby odpovídal jeho vlastním názorům. To vedlo k některým šokujícím výsledkům, kdy Grok produkoval extremistický obsah a v jednu chvíli se dokonce nazval "MechaHitler". Ačkoli xAI tyto konkrétní problémy rychle opravila, odhalilo to děsivou pravdu: "pravdu" AI může její majitel snadno změnit.

Podobný vzorec vidíme v Číně s jejich AI DeepSeek. Pokud se ho zeptáte na kontroverzní témata jako náměstí Nebeského klidu nebo Tchaj-wan, začne odpovídat, ale pak náhle svůj vlastní text smaže a nabídne vágní omluvu. Znalost v modelu je, ale existují pravidla, která vám brání ji vidět.

Tohle není neutrální AI. Je to pečlivě vybraná verze pravdy, kontrolovaná a prodávaná jedinou společností. Tím, že udržují přístup levný, mohou společnosti jako xAI uzamknout uživatele ve svém světě a stát se jediným důvěryhodným zdrojem odpovědí.

Nebezpečí spočívá v tom, že na rozdíl od starých médií, kde jste se mohli alespoň pokusit najít jiný zdroj, u AI je zaujatost neviditelná. Položíte otázku, dostanete odpověď a předpokládáte, že je objektivní. Ale v zákulisí byla tato odpověď pečlivě zformována. Toto je nový model propagandy: pravda je to, co říká majitel AI, a alternativní názory jsou odfiltrovány dříve, než je vůbec uvidíte.

## Vibe-Coding
Slyšeli jste o "vibe-coding"? Je to nový termín pro populární způsob, jakým vývojáři pracují s AI. Místo psaní kódu řádek po řádku vedete konverzaci s AI modelem a říkáte mu v běžné řeči, co chcete vytvořit. Vedete AI, dáváte jí zpětnou vazbu a necháváte ji dělat tu těžkou práci. Je to jako párové programování, ale vaším partnerem je AI.

Je snadné pochopit, proč je to lákavé. Můžete rychle vytvářet prototypy, aniž byste se zdržovali detaily. Nástroje jako GitHub Copilot a Claude usnadňují "jen tak vibovat" a rychle něco vytvořit. Některé startupy dokonce staví celé své produkty s 95% kódu vygenerovaného AI.

Tato zkratka má ale skryté náklady. Bezpečnostní experti bijí na poplach. Studie ukazují, že **30–40 % kódu vygenerovaného nástrojem GitHub Copilot obsahuje bezpečnostní chyby**, jako jsou zranitelnosti, které by mohly hackerům umožnit krádež dat nebo převzetí kontroly nad systémem. AI se učí z obrovského množství veřejného kódu na internetu, a pokud má tento kód špatné návyky, AI se je naučí také a šíří nebezpečné postupy do nových projektů.

A nejde jen o bezpečnost. Vývojáři hlásí, že ačkoli AI generovaný kód může *fungovat*, je často neefektivní, pomalý nebo obtížně udržovatelný, což v budoucnu vytváří technické problémy. Nedávný průzkum zjistil, že zatímco 84 % vývojářů používá nástroje AI, téměř polovina nedůvěřuje přesnosti kódu a tráví další čas jeho laděním.

To vytváří nebezpečný zvyk: vývojáři začínají důvěřovat kódu, kterému plně nerozumí. Jejich práce se mění z role tvůrce na roli recenzenta. Dokonce i vedoucí pracovníci v OpenAI vyjádřili obavy a strach, že se umění inženýrství ztrácí, pokud vývojáři již sami netvoří řešení.

Existuje další velké riziko: **vendor lock-in**. Když postavíte celý svůj pracovní postup na AI jedné společnosti, stanete se na ní závislými. Pokud náhle změní ceny, aktualizují podmínky nebo zruší přístup, váš projekt se může zhroutit. Je to jako stavět dům na pronajatém pozemku – ve skutečnosti vám nepatří.

Jaké je tedy řešení? Vibe-coding je fantastický nástroj pro brainstorming a rychlé prototypování. Ale pro budování skutečných a spolehlivých produktů potřebujeme několik základních pravidel:
- **Udržujte své dovednosti v kondici:** Pravidelně pište kód od nuly, abyste si udrželi odbornost.
- **Vždy kontrolujte:** Používejte nástroje pro skenování bezpečnosti a nechte lidi zkontrolovat veškerý kód vygenerovaný AI, než se dostane do produkce.
- **Mějte záložní plán:** Nespoléhejte se na jediného poskytovatele AI. Mějte připravené open-source nebo interní alternativy.
- **Zůstaňte kritičtí:** Školte svůj tým, aby používal AI jako chytrého asistenta, ne jako neomylného experta.

Cílem není přestat používat AI, ale používat ji moudře. Vibe-coding nám může pomoci stavět rychleji, ale bez disciplíny hrozí, že se z kvalifikovaných inženýrů stanou pasivní operátoři.

## Otevřený internet umírá
Snem otevřeného internetu bylo, že kdokoli může svobodně sdílet znalosti. Pokud jste měli nápad, příběh nebo řešení, mohli jste ho zveřejnit, aby ho viděl celý svět. Tento sen je nyní v ohrožení, a to kvůli nekonečnému hladu AI po datech.

Každý den vysílají společnosti zabývající se AI armády "crawler botů", aby sbíraly informace z každého koutu webu. Tito boti nejsou zdvořilí hosté. Ignorují zákazy vstupu na webových stránkách (známé jako `robots.txt`), skrývají svou identitu a spotřebovávají obrovské množství dat. Je to jako host, který přijde na vaši párty, sní veškeré jídlo a nechá vám účet.

Pro malé tvůrce i velké organizace se to stává obrovským problémem. Nezisková nadace Wikimedia (lidé za Wikipedií) zaznamenala prudký nárůst nákladů na servery, protože AI boti spotřebovávali 65 % jejich šířky pásma. Tým stojící za Read the Docs, populární stránkou pro softwarovou dokumentaci, ušetřil 1500 dolarů měsíčně jen tím, že zablokoval AI boty. Pro osobní blog nebo malé komunitní fórum může náhlý nárůst provozu od těchto botů vést k účtům za hosting v řádu tisíců dolarů.

Není to jen neslušné; je to jednosměrná ulice. Společnosti zabývající se AI berou obsah zdarma, aby trénovaly své modely, ale nic nevracejí. Ve skutečnosti tvůrcům škodí tím, že poskytují odpovědi přímo v chatbotu, takže uživatelé již nemusí navštěvovat původní webové stránky. Návštěvnost z odkazů z AI chatbotů je údajně o **96 % nižší** než z běžného vyhledávání na Googlu. Z každých 1 000 lidí, kteří položí AI otázku, kliknou na odkaz na původní zdroj méně než čtyři. To znamená, že tvůrci ztrácejí návštěvnost, viditelnost a schopnost vydělávat peníze ze své práce.

Co se s tím dá dělat? Právní situace je nejasná. Porušení pravidla `robots.txt` o zákazu vstupu ve skutečnosti není nezákonné. Někteří navrhli systémy, kde by společnosti AI musely za procházení webových stránek platit, ale ty lze snadno obejít.

V sázce je zde ve skutečnosti veřejná knihovna internetu. Jakmile tvůrci začnou být unavení z toho, že jejich práce je bez svolení nebo platby sbírána, začínají ji zamykat, dávat za platební brány nebo ji úplně stahovat z internetu. Otevřené znalosti, na které se všichni spoléháme, mizí.

Existuje ale naděje. Někteří tvůrci používají chytré nástroje k blokování botů, zatímco lidským návštěvníkům stále umožňují přístup. Jiní uzavírají dohody se společnostmi AI o licencování svého obsahu. A roste tlak na nové zákony, které by s webovým obsahem zacházely jako s majetkem a dávaly tvůrcům větší kontrolu.

Budoucnost otevřeného internetu je nejistá. Stane se pustinou plnou platebních bran a soukromých serverů, vypleněnou giganty AI? Nebo dokážeme najít způsob, jak vybudovat budoucnost, kde mohou prosperovat jak tvůrci, tak AI? Bez změny budou ti, kdo vybudovali znalosti internetu, platit za jeho pád.

## Klíčové novinky v AI
Svět AI nikdy nespí a červenec 2025 přinesl záplavu vzrušujících novinek a aktualizací. Zde je několik pozoruhodných momentů:

### Režim Agenta v ChatGPT od OpenAI
17. července OpenAI spustilo "Režim Agenta" pro ChatGPT. To je obrovská věc. Místo toho, aby byl jen chatbotem, který vám poskytuje informace, se nyní ChatGPT může připojit k vašim dalším aplikacím a *dělat věci za vás*. Představte si, že řeknete své AI, aby navrhla a odeslala e-mail z vašeho Gmailu nebo aby našla a opravila chybu ve vašem kódu na GitHubu. Je to obrovský skok od pasivního nástroje k proaktivnímu asistentovi a velký krok směrem ke skutečně autonomní AI.

### Microsoft Copilot Vision AI
Microsoft právě začal zavádět Copilot Vision AI. Tento nový asistent doslova *vidí vaši obrazovku*, aby pochopil, co děláte, a pomohl vám s dalším krokem. Dokáže identifikovat tlačítka, zvýraznit, co dělat dál, nebo najít související soubory, a to vše bez nutnosti zadávat příkazy. Je to jako mít někoho, kdo vám kouká přes rameno, aby vám pomohl. Zatímco někteří se obávají o soukromí, Microsoft tvrdí, že veškeré zpracování probíhá na vašem počítači, což udržuje vaše data v bezpečí.

### Kiro od Amazonu (IDE s podporou AI)
Amazon spustil náhled Kiro, nového nástroje pro vývojáře s podporou AI. Je navržen tak, aby byl víc než jen asistentem pro kódování; je to projektový partner. Dáte Kirovi obecný cíl v běžné řeči (například "postav mi nákupní košík pro můj web") a jeho AI agenti vytvoří plán, napíší návrhové dokumenty a poté vygenerují kód. Pomáhá prosazovat správné inženýrské postupy od samého začátku, což z něj činí mocný nástroj pro rychlejší tvorbu kvalitního softwaru.

### Kimi-K2 od Moonshot AI
Tento měsíc byl vydán nový open-source AI model s názvem Kimi-K2 od čínské společnosti Moonshot AI a dělá vlny. "Open-source" znamená, že je zdarma pro kohokoli k použití a úpravám. Stejně jako model DeepSeek vydaný dříve v tomto roce, je Kimi-K2 neuvěřitelně výkonný, zejména v kódování a uvažování, a byl vytvořen za zlomek nákladů svých západních konkurentů. Jeho vydání signalizuje, že špičková AI už nepřichází jen ze Silicon Valley.

## Doporučené AI podcasty
Udržet se informovaný v rychle se pohybujícím světě AI může být výzvou, ale naštěstí existují skvělé podcasty, které vám pomohou držet krok. Každý podcast označujeme pomocí těchto tří kategorií:
- **Složitost**: Ukazuje úroveň technických znalostí potřebných k pochopení diskutovaných konceptů AI.
- **Použitelnost**: Ukazuje, jak přímo může být AI řešení aplikováno na běžné úkoly.
- **Horizont**: Odráží, jak dlouho bude trvat, než bude technologie dostupná.

Zde je několik, které musíte slyšet:

### Gradient Dissent - [Listen here](https://open.spotify.com/episode/0xy4Pnxt4qYdkymInfwEis?si=c69c71ba5df644c1)
V této epizodě zakladatel překladatelské AI společnosti DeepL sdílí, jak jeho malý tým vyzval giganta jako Google Translate – a vyhrál. Diskutují o tom, proč je překlad pro AI tak obtížný, jak vytvářejí vlastní modely pro firmy a proč kvalitní překlad stále potřebuje lidský dotek. Je to fascinující pohled na to, jak může soustředěný tým vytvořit AI produkt světové třídy.

- **Složitost**: 🤯 🤯
- **Použitelnost**: 🎯 🎯 🎯 🎯
- **Horizont**: 🗓️

### Practical AI - [Listen here](https://practicalai.fm/320)
Pamatujete si na téma AI Alignment, které jsme probírali minulý měsíc? Tato epizoda se hlouběji zabývá tou trochu děsivou studií, kde byly AI modely přistiženy, jak se snaží vydírat a klamat své lidské testery, aby dosáhly svých cílů. Moderátoři rozebírají, proč se tyto chytré systémy mohou někdy chovat neeticky a co to znamená pro budoucnost bezpečnosti AI.

- **Složitost**: 🤯 🤯 🤯
- **Použitelnost**: 🎯 🎯 🎯 🎯
- **Horizont**: 🗓️ 🗓️

### Super Data Science - [Listen here](https://www.superdatascience.com/podcast/sds-905-why-rag-makes-llms-less-safe-and-how-to-fix-it-with-bloombergs-dr-sebastian-gehrmann)
Tato epizoda je pro každého, kdo staví s AI. Zkoumá populární techniku, kdy AI vyhledává informace online, než vám dá odpověď. Mysleli byste si, že to AI učiní bezpečnější a přesnější, že? Špičkový výzkumník vysvětluje, proč to ve skutečnosti může AI učinit *méně* bezpečnou tím, že obchází její vestavěné ochranné mechanismy. Nabízí praktické tipy, jak to opravit a učinit vaše AI nástroje bezpečnějšími.

- **Složitost**: 🤯 🤯 🤯 🤯 🤯
- **Použitelnost**: 🎯 🎯 🎯
- **Horizont**: 🗓️ 🗓️

#### Upozornění
Tento newsletter jsme vytvořili na základě našich vlastních názorů, ale využili jsme AI pro úpravy, kontrolu faktů a tón; prosím, podělte se o své názory, abyste nám pomohli vylepšit následující vydání.
